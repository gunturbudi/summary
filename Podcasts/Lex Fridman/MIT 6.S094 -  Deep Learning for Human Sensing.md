# Summary of MIT 6.S094: Deep Learning for Human Sensing

This video from MIT discusses the use of deep learning for human sensing. The video covers topics such as data collection and annotation, human computation, and the use of deep learning for things like driving and facial recognition.
The "MIT 6.S094: Deep Learning for Human Sensing" video discusses the Deep Learning for Human Sensing course, which explores various deep learning algorithms to help autonomous vehicles better understand humans. The course is open to the public, and participants are encouraged to submit their best speeds.

Detail Summary: 
00:00:00
Today's lecture will discuss how to apply deep learning methods to understanding the sense of human beings, focusing on computer vision. First and foremost, data is essential for success: we need a lot of real world data to train our machine learning models. Once we have the data, we must annotate it in order to unlock its full potential. Finally, we will discuss ways to leverage human computation to achieve success in the real world.

00:05:00
The video discusses how humans are amazing drivers and how this presents a challenge for automated vehicles. It goes on to discuss how humans are also good at certain tasks such as soccer, which presents a challenge for automated vehicles. The last point the video makes is that humans need to be treated as an integral part of the automated vehicle system, and that we need to approach the autonomous vehicle paradigm in a human-centered way.

00:10:00
This video discusses the dangers of texting and driving, and how to avoid being distracted. It also provides a visual representation of how long it takes to look away from the road while texting.

00:15:00
The video discusses the importance of data in autonomous vehicle technology, and how deep learning can be used to improve the accuracy of perception tasks. It also discusses how the data collected from human drivers can be used to train autonomous vehicles.

Human drivers are often irrational when it comes to their risk estimates, which can lead to improper use of autonomous technology. However, deep learning systems are able to learn from data and improve their performance over time. In fact, the video shows that Tesla autopilot is being used on a large scale, and that people are getting value from it.

00:20:00
The video discusses Deep Learning for Human Sensing, specifically how value that is enjoyed, such as when a person gazes at something, can be determined. The algorithm used is the suffocation algorithm, which is used to understand what's in a data set of images and bar graphs. With regard to glance classification, the study found that there is not much of a difference in the distribution of drivers' glances when automated driving is present and when manual driving is present. However, when looking at the aggregate data, there is a significant difference between the two situations. This suggests that people are getting value from using these technologies, but are still attentive or at least physically engaged when their eyes are on the road.

00:25:00
The video discusses how deep learning can be used for human sensing, specifically for localization (e.g., determining the position and orientation of a person or object in an image) and body pose estimation (determining the position and orientation of a person's limbs and body). The video also discusses how deep learning can be used for things like driving, seatbelt testing, and airbag deployment.

00:30:00
This video shows how the MIT 6.S094 course Deep Learning for Human Sensing covers topics such as smartphone detection, estimation of body parts, and body pose estimation. It also explains how to use these techniques to analyze nonverbal communication between pedestrians and vehicles.

00:35:00
This YouTube video explains how deep learning can be used to detect human body behavior, such as hesitation, glass classification, and glance classification. This is important for driving, as it allows the car to determine where the driver is looking.

00:40:00
In this video, the MIT team describes their deep learning algorithms for human sensing, including glass classification and eye tracking. The team notes that while the algorithms are highly accurate in the majority of cases, they rely on human annotation to correct errors.

00:45:00
The course "MIT 6.S094: Deep Learning for Human Sensing" covers the basics of deep learning for human sensing, including how to train a neural network to recognize emotions from raw pixels. This process is key to applications such as autonomous vehicles and facial recognition.

00:50:00
This video discusses how cognitive load affects the way people express emotions through their eyes. The video also includes a brief mention of the AGI class next week.

00:55:00
In this video, the team at MIT describe how they used 3d convolutional neural networks to classify the level of cognitive load during driving. The neural network was able to achieve accuracy of 86%.

01:00:00
The video discusses how deep learning can be used to improve human sensing, and how this is important for future automation. The talk also touches on the importance of data and training for such systems, and highlights how Tesla is leading the way in this area.

01:05:00
The video discusses MIT's Deep Learning for Human Sensing course, which aims to help autonomous vehicles better understand humans. The course is open to the public, and participants are encouraged to submit their best speeds.

01:10:00
The "MIT 6.S094: Deep Learning for Human Sensing" video introduces the Deep Learning for Human Sensing course, which will explore various deep learning algorithms. The course runs for eight weeks in the spring semester and is open to students from a variety of disciplines.

