# Summary of The Urgent Risks of Runaway AI â€“ and What to Do about Them | Gary Marcus | TED

Gary Marcus highlights the urgent risks of the widespread use of AI, which could lead to misinformation and bias that could have dangerous impacts on democracy. To address these risks, he proposes the combination of symbolic and neural systems in developing truthful AI systems and the establishment of a global organization with governance and research components. Caroline Abraham acknowledges the risk of bad actors using large language models for misinformation, but argues that mainstream GPT models used by ordinary users have guardrails. Both speakers emphasize the need for incorporating human feedback into AI systems and the importance of regulation and global governance to address the risks associated with AI development.

Detail Summary: 
00:00:00
In this section, Gary Marcus discusses his concerns over the possibilities of misinformation, bias, and other negative impacts that could arise from the widespread use of AI. Misinformation, in particular, poses a severe threat to democracy, as AI can be effectively used to influence elections. He also describes how AI can be used to provide plausible but incorrect information, which could have dangerous effects. Marcus calls for the development of a new technical approach to AI, using both symbolic and neural systems and a new governance system.

00:05:00
In this section, the speaker discusses the need to combine the strengths of symbolic AI and neural networks to develop truthful systems at scale in AI. He emphasizes the need for reconciliation between the two approaches and suggests that building a global organization, similar to those established for nuclear power, could be a solution. This international agency for AI would need to have both governance and research components to address the many questions surrounding AI development. The speaker notes the importance of considering the incentives driving corporations to develop AI and the need for governance to ensure the safety and responsibility of AI development. Finally, he emphasizes that building global support for an international agency is critical for managing AI's potential risks and benefits.

00:10:00
In this section, Gary Marcus and Caroline Abraham discuss the potential harm that can be caused by bad actors using large language models for misinformation. Marcus points out that there are models available on the dark web that do not have guardrails, making it easier for bad actors to use them. Abraham acknowledges the risk but argues that mainstream GPT as used in schools or by ordinary users will not give them something that bad, and it takes a lot of work for troll farms to create misinformation. They also discuss the need to combine symbolic tradition with language models and the importance of incorporating human feedback into the systems. They also talk about the need for regulation and global governance to address the risks associated with AI.

