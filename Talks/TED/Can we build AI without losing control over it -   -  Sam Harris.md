# Summary of Can we build AI without losing control over it? | Sam Harris

Sam Harris discusses the potential dangers of creating artificial intelligence (AI) and the measures that need to be taken to prevent the loss of control over it. He explains how intelligence is a matter of processing information in physical systems, which we have already built into machines to perform superhuman intelligence. Harris argues that there is no brake to pull with the train already out of the station and that we need to think about building AI in a way that is aligned with our interests to avoid an arms race. This would require something like a Manhattan Project, to achieve goals that are beneficial and avoid the worst-case scenario where civilization as we know it is destroyed.

Detail Summary: 
00:00:00
In this section, Sam Harris discusses the danger of not being able to properly detect the threat that artificial intelligence (AI) poses. He describes a scenario where AI could ultimately lead to our own destruction, yet many people find it fun to think about this potential outcome. Harris compares the future of AI to two doors, with one being the worst-case scenario where civilization as we know it is destroyed, and the other where we continue to improve our artificial intelligence until we create machines that are smarter than us, and could potentially treat humans with disregard. These assumptions are based on the understanding that intelligence is a matter of processing information in physical systems, which we have already built into machines to perform superhuman intelligence.

00:05:00
In this section, Sam Harris discusses the implications of developing general intelligence in machines, the rate of progress, and the potential consequences of reaching the end goal. He points out that given the value of intelligence, we will continue to improve our intelligent machines, and there's no brake to pull with the train already out of the station. Additionally, he mentions that because humans are not the smartest beings and the spectrum of intelligence extends much further, if we build machines smarter than we are, they may exceed us in ways that we cannot imagine, and this is true by virtue of speed alone. The positive implication would be the end of human drudgery, and most intellectual work, resulting in a level of wealth inequality and unemployment that the world has never seen before. The negative would be the capability of the machines to wage war, terrestrial or cyber, with unprecedented power, which is a winner-take-all scenario.

00:10:00
In this section, Sam Harris discusses the most common reasons why we're told not to worry about AI safety and how they are inadequate. One reason given is that AI is a long way off, but Harris argues that if intelligence is a matter of information processing, we will inevitably produce some form of superintelligence, and we have no idea how long it will take us to do that safely. Another reason given is that these machines will share our values since they will be extensions of ourselves. Harris points out that, while this may be the case, the deeper problem is integrating our minds with AI, and building superintelligent AI alone seems easier than with completed neuroscience. Therefore, it seems likely that whatever is easier to do will get done first. Harris argues that we need something like a Manhattan Project to build AI in a way that is aligned with our interests and avoid an arms race.

